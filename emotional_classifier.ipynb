{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, get_scheduler\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sentences_helper import sentence_es, sentence_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"google-bert/bert-base-multilingual-cased\"\n",
    "# model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
    "# model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "model_name = \"FacebookAI/xlm-roberta-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmpatheticDialogues\n",
    "\n",
    "Emotions labels:\n",
    "Surprised, Excited, Angry, Proud, Sad, Annoyed, Grateful, Lonely, Afraid, Terrified, Guilty, Impressed, Disgusted, Hopeful, Confident, Furious, Anxious, Anticipating, Joyful, Nostalgic, Disappointed, Prepared, Jealous, Content, Devastated, Embarrassed, Caring, Sentimental, Trusting, Ashamed, Apprehensive, Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = 'data/MPATHY/MPATHY_translation_en2es.csv'\n",
    "label = 'data/MPATHY/MPATHY_dialoginfo.csv'\n",
    "\n",
    "data_seg = pd.read_csv(seg)\n",
    "data_label = pd.read_csv(label)\n",
    "\n",
    "ED_data = pd.DataFrame(columns=['label', 'text'])\n",
    "ED_data['uid'] = data_seg['UID']\n",
    "ED_data['text'] = data_seg['SEG']\n",
    "ED_data['translation'] = data_seg['translation']\n",
    "\n",
    "# ED_data['label'] = data_label['emotion']\n",
    "row= 0\n",
    "for i in range(len(data_label)):\n",
    "    turns = data_label['turns'][i] + 1 # plus because index 0\n",
    "    for j in range(turns):\n",
    "        ED_data['label'][row+j] = data_label['emotion'][i]\n",
    "    row += turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other - Anticipating/Prepared\n",
    "ED_data.drop(ED_data.loc[ED_data['label'] == 'anticipating'].index[:], inplace=True) # delete rows\n",
    "ED_data.drop(ED_data.loc[ED_data['label'] == 'prepared'].index[:], inplace=True) # delete rows\n",
    "\n",
    "# Remap Emotions\n",
    "\n",
    "# Anger - Angry/Annoyed/Furious/Jealous\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"angry\", \"anger\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"annoyed\", \"anger\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"furious\", \"anger\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"jealous\", \"anger\")\n",
    "\n",
    "# Disgust - Disgusted/\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"disgusted\", \"disgust\")\n",
    "\n",
    "# Fear - Afraid/Anxious/Apprehensive/Terrified\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"afraid\", \"fear\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"anxious\", \"fear\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"apprehensive\", \"fear\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"terrified\", \"fear\")\n",
    "\n",
    "# Happiness - Caring/Confident/Content/Excited/Faithful/Grateful/Joyful/Hopeful/Proud/Trusting\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"caring\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"confident\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"content\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"excited\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"faithful\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"grateful\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"joyful\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"hopeful\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"proud\", \"happiness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"trusting\", \"happiness\")\n",
    "\n",
    "# Sadness - Ashamed/Devastated/Disapointed/Embarrased/Guilty/Lonely/Nostalgic/Sad/Sentimental\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"sad\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"ashamed\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"devastated\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"disappointed\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"embarrassed\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"guilty\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"lonely\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"nostalgic\", \"sadness\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"sentimental\", \"sadness\")\n",
    "\n",
    "# Surprise - Impressed/Surprised\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"impressed\", \"surprise\")\n",
    "ED_data[\"label\"]= ED_data[\"label\"].str.replace(\"surprised\", \"surprise\")\n",
    "\n",
    "ED_data = ED_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ED_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ED_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ED_data = ED_data[['uid', 'label', 'text', 'translation']]\n",
    "ED_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DailyDialog\n",
    "\n",
    "Emotions labels:\n",
    "Anger, Disgust, Fear, Happiness, Sadness, Surprise and Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = 'data/DAILYD/DAILYD_translation_en2es.csv'\n",
    "label = 'data/DAILYD/DAILYD_dialoginfo.csv'\n",
    "\n",
    "data_seg = pd.read_csv(seg)\n",
    "data_label = pd.read_csv(label)\n",
    "\n",
    "DD_data = pd.DataFrame(columns=['label', 'text'])\n",
    "DD_data['uid'] = data_seg['UID']\n",
    "DD_data['text'] = data_seg['SEG']\n",
    "DD_data['translation'] = data_seg['translation']\n",
    "DD_data['label'] = data_label['emotion']\n",
    "\n",
    "# Remap Emotions\n",
    "DD_data[\"label\"]= DD_data[\"label\"].str.replace(\"no emotion\", \"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD_data\n",
    "DD_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_data.loc[DD_data['label'] == 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_data = DD_data[['uid', 'label', 'text', 'translation']]\n",
    "DD_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [ED_data, DD_data]\n",
    "data = pd.concat(frames)\n",
    "data = data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_es = data.copy()\n",
    "data_en = data.copy()\n",
    "data_es['language'] = 'es'\n",
    "data_en['language'] = 'en'\n",
    "data = pd.concat([data_en[['label', 'text', 'language']], data_es[['label', 'translation', 'language']].rename(columns={'translation': 'text'})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "duplicates = data[data.duplicated(subset=['text'])]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "data.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in data\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f'Duplicates in data: {duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/dataset_multi.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSTPROCESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN & TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset_multi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples per class for the test and validation sets\n",
    "num_samples_per_class_test = 250  # Adjust this value as needed\n",
    "num_samples_per_class_val = 250   # Adjust this value as needed\n",
    "\n",
    "# If language is 'multi', ensure balanced datasets for both English and Spanish\n",
    "test_list_en, test_list_es = [], []\n",
    "val_list_en, val_list_es = [], []\n",
    "train_list_en, train_list_es = [], []\n",
    "\n",
    "# Split each class separately to ensure the same class distribution in the test and validation sets\n",
    "for label in df['label'].unique():\n",
    "    class_samples_en = df[(df['label'] == label) & (df['language'] == 'en')]\n",
    "    class_samples_es = df[(df['label'] == label) & (df['language'] == 'es')]\n",
    "    \n",
    "    # Extract the specified number of samples for the test and validation sets\n",
    "    test_data_en = class_samples_en[~class_samples_en.duplicated(subset=['text'])].sample(n=num_samples_per_class_test, random_state=42)\n",
    "    val_data_en = class_samples_en.drop(test_data_en.index).sample(n=num_samples_per_class_val, random_state=42)\n",
    "    test_data_es = class_samples_es[~class_samples_es.duplicated(subset=['text'])].sample(n=num_samples_per_class_test, random_state=42)\n",
    "    val_data_es = class_samples_es.drop(test_data_es.index).sample(n=num_samples_per_class_val, random_state=42)\n",
    "    \n",
    "    # Remaining samples go to the train set\n",
    "    train_data_en = class_samples_en.drop(test_data_en.index).drop(val_data_en.index)\n",
    "    train_data_es = class_samples_es.drop(test_data_es.index).drop(val_data_es.index)\n",
    "            \n",
    "    train_data_en = train_data_en[~train_data_en['text'].isin(test_data_en['text'])]\n",
    "    train_data_es = train_data_es[~train_data_es['text'].isin(test_data_es['text'])]\n",
    "    \n",
    "    test_list_en.append(test_data_en)\n",
    "    val_list_en.append(val_data_en)\n",
    "    test_list_es.append(test_data_es)\n",
    "    val_list_es.append(val_data_es)\n",
    "    train_list_en.append(train_data_en)\n",
    "    train_list_es.append(train_data_es)\n",
    "\n",
    "# Concatenate train, validation, and test datasets and shuffle them\n",
    "train_df_en = pd.concat(train_list_en).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_df_es = pd.concat(train_list_es).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df_en = pd.concat(val_list_en).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df_es = pd.concat(val_list_es).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df_en = pd.concat(test_list_en).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df_es = pd.concat(test_list_es).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Combine English and Spanish datasets\n",
    "train_df = pd.concat([train_df_en, train_df_es]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = pd.concat([val_df_en, val_df_es]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat([test_df_en, test_df_es]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check new distributions\n",
    "print(\"Train Class Distribution:\\n\", train_df['label'].value_counts())\n",
    "print(\"\\nValidation Class Distribution:\\n\", val_df['label'].value_counts())\n",
    "print(\"\\nTest Class Distribution:\\n\", test_df['label'].value_counts())\n",
    "\n",
    "# Save to CSV\n",
    "train_df.to_csv(\"data/train_dataset.csv\", index=False)\n",
    "val_df.to_csv(\"data/val_dataset.csv\", index=False)\n",
    "test_df.to_csv(\"data/test_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by language and label, then count occurrences\n",
    "emotion_counts = train_df.groupby(['language', 'label']).size().unstack(fill_value=0)\n",
    "# Display the counts\n",
    "print(emotion_counts)\n",
    "# Group by language and label, then count occurrences\n",
    "emotion_counts = val_df.groupby(['language', 'label']).size().unstack(fill_value=0)\n",
    "# Display the counts\n",
    "print(emotion_counts)\n",
    "# Group by language and label, then count occurrences\n",
    "emotion_counts = test_df.groupby(['language', 'label']).size().unstack(fill_value=0)\n",
    "# Display the counts\n",
    "print(emotion_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate rows in train_df\n",
    "duplicates = train_df[train_df.duplicated()]\n",
    "\n",
    "# Display the duplicate r\n",
    "print(\"Duplicate Rows in train_df:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Find duplicate rows in test_df\n",
    "duplicates = val_df[val_df.duplicated()]\n",
    "\n",
    "# Display the duplicate ro\n",
    "print(\"Duplicate Rows in val_df:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Find duplicate rows in test_df\n",
    "duplicates = test_df[test_df.duplicated()]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(\"Duplicate Rows in test_df:\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common rows between train_df and val_df\n",
    "common_rows_val = pd.merge(train_df, val_df, on=['text'], how='inner')\n",
    "print(f\"Number of common rows between train_df and val_df: {len(common_rows_val)}\")\n",
    "\n",
    "# Check for common rows between train_df and test_df\n",
    "common_rows_test = pd.merge(train_df, test_df, on=['text'], how='inner')\n",
    "print(f\"Number of common rows between train_df and test_df: {len(common_rows_test)}\")\n",
    "\n",
    "# Display the common rows if any\n",
    "if not common_rows_val.empty:\n",
    "    print(\"Common rows between train_df and val_df:\")\n",
    "    print(common_rows_val)\n",
    "\n",
    "if not common_rows_test.empty:\n",
    "    print(\"Common rows between train_df and test_df:\")\n",
    "    print(common_rows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any row in test_df is equal to any row in train_df\n",
    "common_rows = val_df[val_df['text'].isin(pd.concat(train_list_en + train_list_es)['text'])]\n",
    "\n",
    "# Display the common rows\n",
    "print(\"Common Rows between val_df and train_df:\")\n",
    "print(common_rows)\n",
    "\n",
    "# Check if any row in test_df is equal to any row in train_df\n",
    "common_rows = test_df[test_df['text'].isin(pd.concat(train_list_en + train_list_es)['text'])]\n",
    "\n",
    "# Display the common rows\n",
    "print(\"Common Rows between test_df and train_df:\")\n",
    "print(common_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling Majority Class (Neutral)\n",
    "## Upsampling Minority Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: Downsampling (frac), Upsampling (ratio)\n",
    "\n",
    "# Check initial class distribution\n",
    "print(\"Before Balancing:\\n\", train_df['label'].value_counts())\n",
    "\n",
    "### STEP 1: DOWNSAMPLE ONLY THE NEUTRAL CLASS ###\n",
    "neutral_class = train_df[train_df['label'] == \"neutral\"]\n",
    "\n",
    "frac = 0.6\n",
    "neutral_downsampled_en = neutral_class[neutral_class['language'] == 'en'].sample(frac=frac, random_state=42)\n",
    "neutral_downsampled_es = neutral_class[neutral_class['language'] == 'es'].sample(frac=frac, random_state=42)\n",
    "neutral_downsampled = pd.concat([neutral_downsampled_en, neutral_downsampled_es])\n",
    "\n",
    "### STEP 2: UPSAMPLE OTHER CLASSES ###\n",
    "upsample_ratios = {\n",
    "    \"anger\": 2,\n",
    "    \"fear\": 3,\n",
    "    \"disgust\": 9,\n",
    "    \"happiness\": 1,\n",
    "    \"sadness\": 1,\n",
    "    \"surprise\": 4\n",
    "}\n",
    "\n",
    "# Initialize a list to store upsampled data\n",
    "train_df_upsampled = []\n",
    "\n",
    "# Upsample the specified classes\n",
    "for label, ratio in upsample_ratios.items():\n",
    "    class_samples = train_df[train_df['label'] == label]\n",
    "    train_df_upsampled.append(resample(class_samples, replace=True, n_samples=len(class_samples) * ratio, random_state=42))\n",
    "\n",
    "# Leave the other classes (not upsampled) unchanged\n",
    "for label in train_df['label'].unique():\n",
    "    if label not in upsample_ratios and label != \"neutral\":\n",
    "        class_samples = train_df[train_df['label'] == label]\n",
    "        train_df_upsampled.append(class_samples)\n",
    "\n",
    "### STEP 3: COMBINE DOWNSAMPLED NEUTRAL CLASS WITH UPSAMPLED CLASSES ###\n",
    "train_df_balanced = pd.concat([neutral_downsampled] + train_df_upsampled)\n",
    "\n",
    "# Shuffle the dataset to mix classes\n",
    "train_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check new class distribution\n",
    "print(\"After Balancing:\\n\", train_df_balanced['label'].value_counts())\n",
    "\n",
    "# Update train_df\n",
    "train_df = train_df_balanced\n",
    "\n",
    "# Save the balanced dataset to a CSV file\n",
    "train_df.to_csv(\"data/train_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by language and label, then count occurrences\n",
    "emotion_counts = train_df.groupby(['language', 'label']).size().unstack(fill_value=0)\n",
    "# Display the counts\n",
    "print(emotion_counts)\n",
    "# Group by language and label, then count occurrences\n",
    "emotion_counts = val_df.groupby(['language', 'label']).size().unstack(fill_value=0)\n",
    "# Display the counts\n",
    "print(emotion_counts)\n",
    "# Group by language and label, then count occurrences\n",
    "emotion_counts = test_df.groupby(['language', 'label']).size().unstack(fill_value=0)\n",
    "# Display the counts\n",
    "print(emotion_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_dataset.csv\")\n",
    "val_df = pd.read_csv(\"data/val_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['label'])\n",
    "val_df['label'] = label_encoder.transform(val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save model and tokenizer\n",
    "model_name_save = './classifier_model_multi/final_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(list(examples), truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Apply tokenization correctly\n",
    "train_encodings = tokenize_function(train_df['text'])\n",
    "val_encodings = tokenize_function(val_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch Dataset\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EmotionDataset(train_encodings, train_df['label'].values)\n",
    "val_dataset = EmotionDataset(val_encodings, val_df['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)  # Convert logits to predicted class ids\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average=\"macro\"),\n",
    "        'precision': precision_score(labels, preds, average=\"macro\"),\n",
    "        'recall': recall_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.to(self.args.device) if class_weights is not None else None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights) if self.class_weights is not None else torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps=None, num_warmup_steps=None):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])], \"weight_decay\": self.args.weight_decay},\n",
    "                {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])], \"weight_decay\": 0.0}\n",
    "            ],\n",
    "            lr=self.args.learning_rate\n",
    "        )\n",
    "\n",
    "        num_warmup_steps = int(0.1 * num_training_steps) if num_warmup_steps is None else num_warmup_steps  # 10% Warmup\n",
    "\n",
    "        lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps) # You can choose from 'linear', 'cosine', 'constant' etc.\n",
    "\n",
    "        self.optimizer, self.lr_scheduler = optimizer, lr_scheduler\n",
    "        return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name_save}/results\",  # Where to save model\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save best model each epoch\n",
    "    learning_rate=5e-6,  # Lower learning rate for better fine-tuning\n",
    "    per_device_train_batch_size=32,  # Adjust based on GPU memory (try 8, 16, or 32)\n",
    "    per_device_eval_batch_size=32,  # Same as train\n",
    "    num_train_epochs=3,  # More epochs since we have a large dataset\n",
    "    weight_decay=0.01,  # Regularization to prevent overfitting\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,  # Log every X steps\n",
    "    save_total_limit=2,  # Keep only last 2 models to save space\n",
    "    metric_for_best_model=\"f1\",  # Best checkpoint based on F1-score\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # Disable logging to external platforms (e.g., wandb)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "label_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = torch.tensor(1.0 / label_counts, dtype=torch.float32)\n",
    "class_weights /= class_weights.sum()  # Normalize\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Trainer setup\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,  # Pass the custom compute_metrics function\n",
    "    class_weights=class_weights  # Pass class weights to the custom trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dropout regularization\n",
    "model.config.hidden_dropout_prob = 0.2\n",
    "model.config.attention_probs_dropout_prob = 0.2\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "trainer.create_optimizer_and_scheduler(num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_name_save)\n",
    "tokenizer.save_pretrained(model_name_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save model and tokenizer\n",
    "model_name_save = './classifier_model_multi/final_model-roberta-large-m1-lr-5e-06-cosine'\n",
    "    \n",
    "# Get latest checkpoint\n",
    "checkpoint_dir = model_name_save + '/results'\n",
    "checkpoint_folders = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint')])[-1]\n",
    "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_folders)\n",
    "\n",
    "# Load model from checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Reinitialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=checkpoint_path),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Resume training (now with a fresh optimizer)\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "\n",
    "# Access the trainer state after resuming from checkpoint\n",
    "trainer_state = trainer.state\n",
    "# print(f\"Trainer State: {trainer_state}\")\n",
    "\n",
    "# Now, let's access the log history for detailed metrics for each epoch\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract metrics (train loss, eval loss, accuracy, etc.) from log_history\n",
    "epochs = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "accuracy = []\n",
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for log in log_history:\n",
    "    # Extracting relevant metrics from the logs\n",
    "    if 'epoch' in log:\n",
    "        epochs.append(log['epoch'])\n",
    "    if 'loss' in log:\n",
    "        train_loss.append(log['loss'])\n",
    "    if 'eval_loss' in log:\n",
    "        val_loss.append(log['eval_loss'])\n",
    "    if 'eval_accuracy' in log:\n",
    "        accuracy.append(log['eval_accuracy'])\n",
    "    if 'eval_f1' in log:\n",
    "        f1.append(log['eval_f1'])\n",
    "    if 'eval_precision' in log:\n",
    "        precision.append(log['eval_precision'])\n",
    "    if 'eval_recall' in log:\n",
    "        recall.append(log['eval_recall'])\n",
    "\n",
    "# If they don't match, adjust the lengths to be consistent:\n",
    "# Trim to the minimum length\n",
    "min_len = min(len(epochs), len(train_loss), len(val_loss), len(accuracy), len(f1), len(precision), len(recall))\n",
    "\n",
    "epochs = epochs[:min_len]\n",
    "train_loss = train_loss[:min_len]\n",
    "val_loss = val_loss[:min_len]\n",
    "accuracy = accuracy[:min_len]\n",
    "f1 = f1[:min_len]\n",
    "precision = precision[:min_len]\n",
    "recall = recall[:min_len]\n",
    "\n",
    "# Now, you should be able to plot the metrics without dimension mismatch\n",
    "fig, axs = plt.subplots(3, 1, figsize=(5, 10))\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "axs[0].plot(epochs, train_loss, label=\"Training Loss\", marker='o')\n",
    "axs[0].plot(epochs, val_loss, label=\"Validation Loss\", marker='o')\n",
    "axs[0].set_title('Training and Validation Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "axs[1].plot(epochs, accuracy, label=\"Accuracy\", marker='o', color='g')\n",
    "axs[1].set_title('Accuracy over Epochs')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot F1, Precision, and Recall\n",
    "axs[2].plot(epochs, f1, label=\"F1 Score\", marker='o', color='b')\n",
    "axs[2].plot(epochs, precision, label=\"Precision\", marker='o', color='r')\n",
    "axs[2].plot(epochs, recall, label=\"Recall\", marker='o', color='orange')\n",
    "axs[2].set_title('F1, Precision, and Recall over Epochs')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('Score')\n",
    "axs[2].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_save)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual test example\n",
    "def predict_emotion(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "    \n",
    "    # Move inputs to the correct device (CPU/GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get logits and apply softmax to get probabilities\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get predicted label and confidence score\n",
    "    predicted_class_id = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence_score = probabilities[0, predicted_class_id].item()\n",
    "    \n",
    "    # Decode the label\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n",
    "    \n",
    "    return predicted_label, confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['Hola Ray, ¿Qué tal estás?', 'Hola Ray, ¿Que tal estas', 'Hola Ray, que tal estas', 'Hola Ray, ¿Cómo estás?']\n",
    "count = 0\n",
    "for text in sentence:\n",
    "    predicted_label, confidence = predict_emotion(text)\n",
    "    print(f\"Text: {text}\\nPredicted: {predicted_label}\\nConfidence: {confidence:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence_es\n",
    "count = 0\n",
    "for s in sentence:\n",
    "    text = s[1]\n",
    "    predicted_label, confidence = predict_emotion(text)\n",
    "    # print(f\"Text: {text}\\nTarget: {s[0]}\\nPredicted: {predicted_label} (Confidence: {confidence:.4f})\\n\")\n",
    "    if s[0] == predicted_label:\n",
    "        count += 1\n",
    "total = count / len(sentence) * 100\n",
    "print(f\"Total Accuracy es: {total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence_en\n",
    "count = 0\n",
    "for s in sentence:\n",
    "    text = s[1]\n",
    "    predicted_label, confidence = predict_emotion(text)\n",
    "    # print(f\"Text: {text}\\nTarget: {s[0]}\\nPredicted: {predicted_label} (Confidence: {confidence:.4f})\\n\")\n",
    "    if s[0] == predicted_label:\n",
    "        count += 1\n",
    "total = count / len(sentence) * 100\n",
    "print(f\"Total Accuracy en: {total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix analysis function\n",
    "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    \n",
    "    # Create annotations for the heatmap\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    \n",
    "    # Convert confusion matrix into DataFrame\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'True'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax, cmap='Oranges')\n",
    "    # plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test_dataset.csv\")\n",
    "y_test = test_df['label'].tolist()\n",
    "\n",
    "# Get true labels, predicted labels, and confidence scores for the test set\n",
    "predictions = [predict_emotion(test) for test in test_df['text']]\n",
    "y_pred, confidence_score = zip(*predictions)\n",
    "\n",
    "# Get the list of class labels from the test dataset\n",
    "labels = test_df['label'].value_counts().index.tolist()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = sum([1 for true, pred in zip(y_test, y_pred) if true == pred]) / len(y_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy: %.3f %%\" % (accuracy * 100))\n",
    "print(\"F1 Score: %.4f\" % f1)\n",
    "print(\"Precision Score: %.4f\" % precision)\n",
    "print(\"Recall Score: %.4f\" % recall, \"\\n\")\n",
    "\n",
    "# Call the confusion matrix analysis function\n",
    "cm_analysis(y_test, y_pred, labels, ymap=None, figsize=(10,10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOECP2vnyEAsLtpmXC3FNKP",
   "collapsed_sections": [
    "O4wAgbmZrwBb",
    "Y9YhoXEwMbwU",
    "qPjGDNKKpvAn",
    "rWlQHdqTpyC3",
    "MOKBfVp6Vgi4",
    "w0okv3PyxfmM",
    "fH_0AE88YVST",
    "bXfpILvRqXDP",
    "M-xehfYcYShj",
    "vyhmu-lp5sXq",
    "Z9t7cqai19Os",
    "AxmxN5cTeUZv",
    "TMZSqmW7VXaz",
    "RJ53hl-GjnFJ",
    "3-_d1el65Ye_",
    "rMqNj2LxYPc7",
    "_7eRx-E2TQN8",
    "iCtcu80wrloY"
   ],
   "machine_shape": "hm",
   "mount_file_id": "1evX8HvkMmCIfbFQ0tLJm7l8pHSfwMAe0",
   "name": "emotional_classifier.ipynb",
   "provenance": [
    {
     "file_id": "1HOd_Tyv74pWBKUwDPuGMsTdCSZKWiWxe",
     "timestamp": 1587665709655
    }
   ]
  },
  "kernelspec": {
   "display_name": ".vgemma3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "161d0666ad2a458ab7d5349e7320a9ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb24246af2094f3c91e0b024bf168788",
      "placeholder": "​",
      "style": "IPY_MODEL_27607772fa7b44c7930b13a024172e98",
      "value": " 483/483 [00:00&lt;00:00, 14.0kB/s]"
     }
    },
    "23d49b3f813f4bd591ae01f49732bb6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2607104a64354ebd809b38c9871558f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7184b8f8386a4d00bcff15a76a04e262",
       "IPY_MODEL_934007cbcb6c4e429b0279ccb4061165",
       "IPY_MODEL_161d0666ad2a458ab7d5349e7320a9ea"
      ],
      "layout": "IPY_MODEL_e3f06e25c283475a863d0f923131aafa"
     }
    },
    "27607772fa7b44c7930b13a024172e98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "346cb586a55543f79f967f62f03a6a84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7184b8f8386a4d00bcff15a76a04e262": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_346cb586a55543f79f967f62f03a6a84",
      "placeholder": "​",
      "style": "IPY_MODEL_23d49b3f813f4bd591ae01f49732bb6e",
      "value": "Downloading: 100%"
     }
    },
    "8c39f46f0eea4a0bb621fd12c247f2af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "934007cbcb6c4e429b0279ccb4061165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c39f46f0eea4a0bb621fd12c247f2af",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6c8cda66e3e4a8fb041e3c516fac709",
      "value": 483
     }
    },
    "d6c8cda66e3e4a8fb041e3c516fac709": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e3f06e25c283475a863d0f923131aafa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb24246af2094f3c91e0b024bf168788": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
